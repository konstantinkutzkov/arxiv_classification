{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import *\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import TFAutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ff968",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/home/koki/Desktop/Data/NLP/arxiv/archive/arxiv-metadata-oai-snapshot.json'\n",
    "            #'/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\n",
    "labelmap = {}\n",
    "sample_rate = 1 # sample rate in percentage, 1% is just for testing  \n",
    "abstracts_train, labels_train, labelmap = get_data_and_labels(\n",
    "                    datapath=datapath,\n",
    "                    year=2021, \n",
    "                    month_start=1,\n",
    "                    month_end=12,\n",
    "                    labelmap=labelmap,\n",
    "                    update_map=True,\n",
    "                    sample_rate=sample_rate\n",
    "                    )\n",
    "\n",
    "abstracts_val, labels_val, _ = get_data_and_labels(\n",
    "                    datapath=datapath,\n",
    "                    year=2022, \n",
    "                    month_start=1,\n",
    "                    month_end=6,\n",
    "                    labelmap=labelmap,\n",
    "                    update_map=False,\n",
    "                    sample_rate=sample_rate\n",
    "                    )\n",
    "\n",
    "abstracts_test, labels_test, _ = get_data_and_labels(\n",
    "                    datapath=datapath,\n",
    "                    year=2022, \n",
    "                    month_start=7,\n",
    "                    month_end=12,\n",
    "                    labelmap=labelmap,\n",
    "                    update_map=False,\n",
    "                    sample_rate=sample_rate\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b95ff6",
   "metadata": {},
   "source": [
    "### Loading a Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54bb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname= 'bert-base-uncased'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "bert_model = TFAutoModel.from_pretrained(modelname) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79336e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512 # for scalability\n",
    "def bert_tokenize(sentence):\n",
    "    tokens = bert_tokenizer.encode_plus(sentence, max_length=SEQ_LEN,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_attention_mask=True,\n",
    "                                   return_token_type_ids=False, return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698586aa",
   "metadata": {},
   "source": [
    "### Tokenization and computing the attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids_and_masks(abstracts):\n",
    "# initialize two arrays for input tensors\n",
    "    Xids = np.zeros((len(abstracts), SEQ_LEN))\n",
    "    Xmask = np.zeros((len(abstracts), SEQ_LEN))\n",
    "\n",
    "    for i, sentence in enumerate(abstracts):\n",
    "        if i % 5000 == 0:\n",
    "            print('#  processed documents', i)\n",
    "        Xids[i, :], Xmask[i, :] = bert_tokenize(sentence)\n",
    "    return Xids, Xmask\n",
    "\n",
    "Xids_train, Xmask_train = get_ids_and_masks(abstracts_train)\n",
    "Xids_val, Xmask_val = get_ids_and_masks(abstracts_val)\n",
    "Xids_test, Xmask_test = get_ids_and_masks(abstracts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to create a tensorflow dataset\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b9798",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_classes = np.max([l for lb in labels_train for l in lb]) + 1 #labels are consecutive integers starting at 0\n",
    "nr_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels, nr_classes=nr_classes):\n",
    "    enc_labels = np.zeros((len(labels), nr_classes))  # initialize empty (all zero) label array\n",
    "    for idx, label in enumerate(labels):\n",
    "        for lb in label:\n",
    "            enc_labels[idx, lb] = 1  # add ones in indices where we have a value\n",
    "    return enc_labels\n",
    "\n",
    "enc_labels_train = encode_labels(labels_train)\n",
    "enc_labels_val = encode_labels(labels_val)\n",
    "enc_labels_test = encode_labels(labels_test)\n",
    "\n",
    "assert enc_labels_train.shape[1] == enc_labels_val.shape[1]\n",
    "assert enc_labels_train.shape[1] == enc_labels_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(Xids, Xmask, enc_labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, enc_labels))\n",
    "    dataset = dataset.map(map_func)\n",
    "    dataset = dataset.batch(64) #dataset.shuffle(1000).batch(32)\n",
    "    return dataset\n",
    "\n",
    "train = get_dataset(Xids_train, Xmask_train, enc_labels_train)\n",
    "val = get_dataset(Xids_val, Xmask_val, enc_labels_val)\n",
    "test = get_dataset(Xids_test, Xmask_test, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5a757",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(SEQ_LEN,), name='attention_mask', dtype='int32')\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(SEQ_LEN,), name='attention_mask', dtype='int32')\n",
    "\n",
    "    embeddings = bert_model(input_ids, attention_mask=mask)[0]  # we only keep last_hidden_state\n",
    "\n",
    "    print(embeddings.shape)\n",
    "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality\n",
    "    #X = tf.keras.layers.AveragePooling1D()(embeddings)\n",
    "    print(X.shape)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Dense(128, activation='leaky_relu')(X)\n",
    "    X = tf.keras.layers.Dropout(0.3)(X)\n",
    "    y = tf.keras.layers.Dense(nr_classes, activation='sigmoid', name='outputs')(X)  # adjust based on number of sentiment classes\n",
    "    \n",
    "    mlp_model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "    #mlp_model.layers[2].trainable = False\n",
    "    \n",
    "    # freezing the Bert layer \n",
    "    bert_layer = [layer.name for layer in mlp_model.layers if 'tf_bert' in layer.name]\n",
    "    mlp_model.get_layer(bert_layer[0]).trainable=False\n",
    "    \n",
    "    return mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b466bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary if we want to create a new model with a clean state\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2691dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = get_mlp_model()\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the F1 metrics defined on tensors, might be used for early stopping\n",
    "\n",
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1074ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "losses = [tf.keras.losses.BinaryCrossentropy(), tf.keras.losses.BinaryFocalCrossentropy()]\n",
    "loss = losses[1]\n",
    "acc = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "auc = tf.keras.metrics.AUC()\n",
    "\n",
    "mlp_model.compile(optimizer=optimizer, loss=loss, metrics=[macro_f1, auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c33dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to store the model in a Kaggle \n",
    "!mkdir -p training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e964a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/kaggle/working/training/mlp_focal_cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8194907",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=4, monitor='val_auc', \\\n",
    "                                            mode='max', restore_best_weights=True)\n",
    "history = mlp_model.fit(train, validation_data=val, callbacks=[es_callback, cp_callback], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "losses = [tf.keras.losses.BinaryCrossentropy(), tf.keras.losses.BinaryFocalCrossentropy()]\n",
    "loss = losses[1]\n",
    "acc = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "auc = tf.keras.metrics.AUC()\n",
    "\n",
    "lstm_model.compile(optimizer=optimizer, loss=loss, metrics=[macro_f1, auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe58010",
   "metadata": {},
   "source": [
    "### A model with an LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8d542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(SEQ_LEN,), name='attention_mask', dtype='int32')\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(SEQ_LEN,), name='attention_mask', dtype='int32')\n",
    "\n",
    "    embeddings = bert_model(input_ids, attention_mask=mask)[0]  # we only keep tensor 0 (last_hidden_state)\n",
    "\n",
    "    print(embeddings.shape)\n",
    "    X = tf.keras.layers.LSTM(256, kernel_initializer='random_normal', return_sequences=False)(embeddings)\n",
    "\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    #X = tf.keras.layers.Dense(32, activation='relu')(X)\n",
    "    X = tf.keras.layers.Dropout(0.3)(X)\n",
    "    y = tf.keras.layers.Dense(nr_classes, activation='softmax', name='outputs')(X)  # adjust based on number of sentiment classes\n",
    "\n",
    "    lstm_model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "    #lstm_model.layers[2].trainable = False\n",
    "    lstm_model.get_layer('tf_bert_model').trainable=False\n",
    "    \n",
    "    return lstm_model\n",
    "\n",
    "lstm_model = get_lstm_model()\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d4305",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "idx = 1\n",
    "losses = [tf.keras.losses.BinaryCrossentropy(), tf.keras.losses.BinaryFocalCrossentropy()]\n",
    "loss = losses[idx]\n",
    "acc = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "auc = tf.keras.metrics.AUC()\n",
    "\n",
    "lstm_model.compile(optimizer=optimizer, loss=loss, metrics=[macro_f1, auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb94cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a suitable name\n",
    "checkpoint_paths = [\"/kaggle/working/training_1/cp_lstm_cross_entropy.ckpt\", \\\n",
    "                    \"/kaggle/working/training_1/cp_lstm_focal_loss.ckpt\"]\n",
    "checkpoint_dir = os.path.dirname(checkpoint_paths[idx])\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4702f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=4, monitor='val_macro_f1', \\\n",
    "                                            mode='max', restore_best_weights=True)\n",
    "history = lstm_model.fit(train, validation_data=val, callbacks=[es_callback, cp_callback], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
